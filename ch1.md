# 1장. 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션
- 오늘날 많은 애플리케이션은 **계산 중심**보다는 **데이터 중심**
- 데이터 중심 애플리케이션은 다음을 필요로 함
    - 데이터베이스: 애플리케이션에서 나중에 데이터를 다시 찾을수 있게 데이터 저장
    - 캐시: 읽기 속도 향상을 위해 값비싼 수행 결과를 IO가 빠른 기억 장치에 저장
    - 검색 색인: 키워드로 데이터 검색하거나 검색 필터링
    - 스트림 처리: 비동기 처리를 위해 다른 프로세스로 메시지 보내기
    - 일괄 처리: 주기적으로 대량의 누적 데이터 분석
- 데이터 시스템의 성공적인 추상화 덕에 표준 구성요소로 데이터 애플리케이션 만듬
    - 엔지니어 입장에서 데이터 시스템 설계 용이
    - 그러나 현실에서는 애플리케이션마다 요구사항 다름
- 데이터 시스템의 원칙과 실용성
# 데이터 시스템
- DB, 큐, 캐시 등은 다른 범주에 속하는 도구
- 이러한 도구들을 묶어 데이터 시스템이라고 함
- 최근의 도구들은 다양한 사용 사례에 최적화되어 분류 간 경계가 흐려지고 있음
    - 메시지 큐의 경우 데이터스토어로 사용하는 Redis가 있는 반면 지속성을 보장하는 Apache Kafka도 있음
- 단일 도구로는 데이터 처리와 저장 모두 만족 불가능할 정도로 광범위한 요구사항 존재
    - work를 단일 도구에서 효율적으로 수행할 수 있는 task로 나누고 각 task를 수행할 수 있는 도구들은 애플리케이션 코드를 이용해 서로 연결
- 애플리케이션 개발자는 데이터 시스템 설계자이기도 함
# 신뢰성
- HW/SW 결함이나 human error에도 시스템이 지속적으로 올바르게 동작해야함
- 소프트웨어에 대한 일반적인 기대치는 다음과 같음
    - 애플리케이션은 사용자가 기대한 기능 수행 - Functionality
    - 시스템은 사용자가 범한 실수나 예상치 못한 사용법을 허용 할 수 있음 - Exception Handling
    - 시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족 - Throughput
    - 시스템은 허가되지 않은 접근과 오남용을 방지 - Security
- 무언가 잘못되더라도 위 기대치가 만족되는것이 신뢰성이라고 볼 수도 있음
## 결함
- 결함이란 잘못될 수 있는 일
- 결함을 예측/대처할 수 있는 시스템은 fault-tolerant 혹은 resilient
- 결함 ≠ 장애
    - 결함은 시스템의 한 구성 요소
    - 장애는 사용자에게 필요한 서비스를 제공 못하고 시스템 전체가 멈춘 경우
    - 따라서 장애란 결함으로 인한 결과
- fault-tolerant한 구조를 설계하여 **신뢰할 수 없는 여러 부품으로 신뢰할 수 있는 시스템 구축**
- 경고 없이 프로세스를 무작위로 죽여 고의적으로 결함을 일으켜 결함률을 증가시키는 방법은 생각보다 납득 가능
    - 미흡한 오류처리보다 고의적인 결함 유도를 통해 자연적인 결함 발생 때 올바르게 처리할 수 있게 하기 위해 시스템에 대해 지속적인 훈련 및 테스트
    - 넷플릭스 - 카오스 몽키
- 예방책이 해결책보다 더 좋은 경우 존재
    - 보안 문제의 경우 공격자가 시스템을 손상시켜 민감한 데이터에 대한 접근 권한을 언든다면 이를 되돌릴 수 없음
### 하드웨어 결함
- 규모가 큰 데이터센터에서는 늘상 일어나는 일
- 구성 요소에 중복을 추가하여 하드웨어 결함으로 인한 시스템 장애율 줄임
    - RAID 디스크
        - 저렴한/독립적인 여러 디스크를 엮어 하나의 디스크처럼 사용하는 방법
    - 이중 전원 디바이스
    - hot-swap 가능한 CPU
    - 예비 전원용 발전기
- 단일 장비의 전체 장애가 드물었을 때는 HW 구성 요소 중복으로 충분했음
    - 장비 백업으로 인한 중단시간은 치명적이지 않음
    - HA가 필수적인 소수의 경우에만 다중 장비 중복 필요
- 데이터 양과 계산 요구가 늘어나면서 더 많은 수의 장비를 사용하고 이에 따라 HW 결함율 증가
    - AWS의 경우 장비 인스턴스 별도 경고 없이 사용 못하는 상황이 일반적으로 발생
    - 이런 플랫폼은 단일 장비 신뢰성보다 유연성, 탄력성 우선적으로 설계
- 일반적으로 하드웨어 결함은 독립적이거나 약한 상관관계
    - 다수의 구성 요소에 동시 장애 발생x
### 소프트웨어 오류
- 시스템 내 체계적 오류(systemiatic error)
    - 노드 간 상관관계 때문에 HW 결함보다 시스템 오류를 더 많이 유발
    - 특정 입력이 있을 때 모든 어플리케이션 서버 인스턴스가 죽는 경우
    - 공유 자원을 과도하게 사용하는 일부 프로세스
    - 시스템 속도가 느려져 반응이 없거나 잘못된 응답 반환하는 서비스
    - 한 구성 요소의 작은 결함이 다른 구성 요소의 결함을 야기하여 더 많은 결함이 발생하는 연쇄 장애(cascading failure)
- 소프트웨어 결함을 유발하는 버그는 특정 상황에 의해 발생하기 전까지 오랫동안 나타나지 않음
    - 시스템 가정과 상호작용에 대한 주의깊은 생각
    - 빈틈없는 테스트
    - 프로세스 격리(process isolation)
    - 죽은 프로세스 재시작 허용
    - 프로덕션 환경에서 시스템 동작 측정, 모니터링, 분석
### 인적 오류
- 운영자 설정 오류가 중단의 주요 원인
- 추상화, API, 관리 인터페이스 사용하여 오류 가능성 최소화
    - 그러나 인터페이스가 지나치게 제한적이면 제한된 인터페이스를 피해 작업, 균형 맞추기 어려움
- 실제 데이터를 사용해 안전하게 실험할 수 있으면서 실 사용자에게 영향 없는 비 프로덕션 sandbox 제공
- 철저한 테스트
- 인적 오류 빠르고 쉽게 복구할 수 있게 하라
    - 롤백, 롤아웃, 데이터 재계산 도구
- 모니터링 댗책
- 조작 교육 및 실습
### 신뢰성은 얼마나 중요할까?
신뢰성은 일상적인 애플리케이션에서도 매우 중요, 사용자에 대한 책임
# 확장성
- 증가한 부하에 대초하는 시스템 능력
- 시스템이 현재 안정적으로 동작한다고 해서 미래에도 안정적이라는 보장x
    - 이용자가 1만에서 100만명으로 증가한다면?
- "추가 부하에 따른 대처 방법은 무엇인가?" 와 같은 질문 고려
## 부하 기술하기
### 부하 매개변수(load parameter)로 부하를 기술
- 적합한 부하 매개변수의 선택은 시스템 설계에 따라 달라짐
    - 초당 요청 수
    - DB Read/Write 비율
    - 동시 활성 사용자
    - Cache hit rate
    - 평균 or 극값
- 트위터
    - 초당 쓰기 처리는 쉽지만 트위터의 확장성 문제는 트윗 양이 아닌 fan-out
        - fan-out이란 하나의 수신 요청을 처리하는데 필요한 다른 서비스의 요청 수
    - 사용자들의 팔로우-팔로잉 관계 구현을 하는 방법은 두가지
        1. 새로운 트윗을 트윗 전역 컬렉션에 삽입하는 방식. 테이블 JOIN하여 팔로우한 모든 사람에 대한 트윗을 찾아 시간순 정렬하여 홈 타임라인 구현
        2. 사용자 별 홈 타임라인 캐시 유지하고 사용자가 트윗을 작성하면 해당 사용자의 팔로워들에 대한 캐시 업데이트.
            - Write 부하는 높아지지만 Read 부하가 낮아짐
            - JOIN 연산이 없음
    - 접근 방식 1은 홈 타임라인 질의 부하를 버텨내기 어려워 2로 전환
    - 팔로워 수가 매우 많다면? - 평균 75명의 팔로워, 초당 4.6k 트윗, 최대 3천만 팔로워
    - 두 접근 방식 hybrid
        - 기본적으로 방식 2
        - 팔로워가 많은 사용자의 트윗은 별도로 가져와 접근 방식 1처럼 읽는 시점에서 홈 타임라인에 합침
            - 여기서 읽을 때 다시 캐시에 팔로워 많은 사용자의 트윗 추가한다면?
## 성능 기술하기
- 부하 매개변수 vs 리소스 vs 성능 비교 해야할 필요가 있음
    - 성능 기술할 필요가 있음
- 하둡 - throughput
- 온라인 시스템 - response time
- response time은 클라이언트 관점, 지연 시간은 요청이 처리되길 기다리는 시간
    - response time은 단일 숫자가 아닌 분포
    - outlier 중요
    - 네트워크나 OS, HW 이슈 등 다양한 원인에 따른 추가 지연 있음
    - 평균보다는 백분위
    - 상위 백분위 응답 시간은 중요
        - 예를 들어 아마존의 경우 99.9분위로 기술, 응답 시간이 느린 요청을 경험한 고객은 많은 구매를 하여 데이터가 많다는 뜻이기에 소중한 고객
    - 응답시간에 따른 판매량 반비례 관계 관찰 가능
    - 99.99분위 최적화 작업은 이득에 비해 비용이 너무 많이 듬
    - SLO, SLA - 응답 시간 백분위 일정 수준 못지키면 고객은 환불 요구 가능
- 서버는 한번에 소수의 작업만 처리 할 수 있기에 소수의 느린 요청이 후속 요청 처리 지체 유발 - 선두차단(head-of-line blocking)
- 시스템 확장성 테스트 시 응답 시간과 독립적으로 부하를 만들어내야함
    - 이전 요청이 완료되길 기다리면 결과 왜곡
- 실전 백분위
    - 병렬로 호출되는 요청의 경우 가장 느린 하나의 요청으로 인해 전체 요청을 느리게함 - 꼬리 지연 증폭
    - 효율성 감안한 응답 시간 백분위 모니터링에 대한 구현 존재
        - 롤링 윈도우
        - 포워드 디케이
        - T 다이제스트
        - Hdr히스토그램
## 부하 대응 접근 방식
- 용량 확장(scaling up)/수직 확장(vertical scaling): 장비 성능 업그레이드
- 규모 확장(scaling out)/수평 확장(horizontal scaling): 장비 개수 증가 -  비공유 아키텍처
- 일반적으로 규모 확장 - 더 저렴한 선택이기 때문
- stateful 데이터 시스템 분산 설치는 stateless에 비해 복잡도 높음
    - stateful할수록 수직확장 용이(ex. 세션, 마스터 노드)
    - 고가용성 요구 있을 때 까지 단일 노드에 DB 유지하는 방향이 통념(용량 확장)
    - 분산 시스템에 발전으로 인해 이 통념이 바뀌기 시작
- 요구 사항에 따라 설계 다름
    - 빈번한 가벼운 연산 vs 간헐적인 무거운 연산
# 유지보수성
- 비용의 대부분은 유지보수
- 레거시 서프트웨어를 만들지 않게 필요한 시스템 설계 원칙 세가지
    1. 운용성(operability): 원활하게 운영할 수 있는 시스템
    2. 단순성(simplicity): 복잡도를 최대한 제거하여 새로운 엔지니어가 이해하기 쉬운 시스템
    3. 발전성(evolvability): 엔지니어가 쉽게 변경할 수 있는 시스템
## 운용성
- 시스템 모니터링
- 보안 패치를 포함한 최신 상태 유지
- 운영 모범 사례 만들기
## 단순성
- 추상화
    - 재사용 가능한 구성 요소
## 발전성
- 애자일
# 정리
## 가능적 요구사항
- 데이터 저장, 조회, 검색, 처리
## 비기능적 요구사항
- 보안, 법규 준수
### 신뢰성
- 하드웨어 결함
- 소프트웨어 버그
### 확장성
- 부하가 증가해도 좋은 성능을 유지하기 위한 전력
### 유지보수성
- 운영 개선